{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import bitsandbytes as bnb\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType, PeftConfig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T03:54:22.708591Z",
     "start_time": "2024-09-15T03:54:22.702587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Bitsandbytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ],
   "id": "b2b7a2494fd7f30",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T03:54:22.789062Z",
     "start_time": "2024-09-15T03:54:22.710099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.\\\\output',  # 模型输出路径\n",
    "    num_train_epochs=3,  # 训练周期\n",
    "    per_device_train_batch_size=2,  # 每个设备上的批量大小，减小显存占用\n",
    "    gradient_accumulation_steps=8,  # 梯度累积步数\n",
    "    learning_rate=5e-5,  # 学习率\n",
    "    fp16=True,  # 混合精度训练（减少显存占用）\n",
    "    logging_dir='.\\\\logs',  # 日志路径\n",
    "    logging_steps=10,  # 日志记录频率\n",
    "    save_steps=500,  # 保存模型频率\n",
    "    save_total_limit=2,  # 最多保存模型数量\n",
    "    load_best_model_at_end=False,\n",
    "    save_strategy=\"steps\"\n",
    ")"
   ],
   "id": "66d909476d7c2f75",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T03:54:22.793899Z",
     "start_time": "2024-09-15T03:54:22.790070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义数据处理函数，将输入文本转换为模型输入\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['input'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = tokenizer(examples['target'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n"
   ],
   "id": "8f0aafc0a594d390",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T03:54:22.803318Z",
     "start_time": "2024-09-15T03:54:22.795405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 量化模型不支持直接进行微调,使用PEFT进行微调(LoRA)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ],
   "id": "46ccb02af38b5cc9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T03:54:38.575347Z",
     "start_time": "2024-09-15T03:54:22.804327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查是否有GPU可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载微调前的模型和tokenizer\n",
    "model_name = \"THUDM/glm-4-9b-chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ],
   "id": "56689176ed10394f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "C:\\Users\\ROG\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\utils\\modeling.py:1462: UserWarning: Current model requires 16.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# 加载微调前的模型和tokenizer\u001B[39;00m\n\u001B[0;32m      5\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTHUDM/glm-4-9b-chat\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 6\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m     13\u001B[0m model \u001B[38;5;241m=\u001B[39m get_peft_model(model, peft_config)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:559\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    557\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    558\u001B[0m         \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mregister(config\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, model_class, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 559\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    561\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:3909\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3906\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m infer_auto_device_map(model, dtype\u001B[38;5;241m=\u001B[39mtarget_dtype, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_map_kwargs)\n\u001B[0;32m   3908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 3909\u001B[0m         \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3911\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3912\u001B[0m     model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:86\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     82\u001B[0m     device_map_without_lm_head \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     83\u001B[0m         key: device_map[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m device_map\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodules_to_not_convert\n\u001B[0;32m     84\u001B[0m     }\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisk\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m---> 86\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     87\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     89\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     90\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`from_pretrained`. Check \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     91\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     92\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor more details. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     93\u001B[0m         )\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbitsandbytes\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m<\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.39.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     97\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     98\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     99\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "train_dataset = load_dataset(\"json\", data_files=\"jsondata\\\\data_1.json\")['train']\n",
    "\n",
    "# 处理数据集\n",
    "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 使用Trainer进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "trainer.save_model('.\\\\output')\n"
   ],
   "id": "75372ca859a408e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
